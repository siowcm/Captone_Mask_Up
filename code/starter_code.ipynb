{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc999c7d-27be-4635-81b4-f1c233cb7572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:01:41.785891Z",
     "iopub.status.busy": "2022-02-07T14:01:41.784820Z",
     "iopub.status.idle": "2022-02-07T14:01:45.844071Z",
     "shell.execute_reply": "2022-02-07T14:01:45.843804Z",
     "shell.execute_reply.started": "2022-02-07T14:01:41.785819Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd989aa-cfab-45e4-9f05-4534c2c6a6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:09:30.932053Z",
     "iopub.status.busy": "2022-02-07T14:09:30.930954Z",
     "iopub.status.idle": "2022-02-07T14:09:30.947730Z",
     "shell.execute_reply": "2022-02-07T14:09:30.946996Z",
     "shell.execute_reply.started": "2022-02-07T14:09:30.932006Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_box(obj):\n",
    "\n",
    "    xmin = int(obj.find(\"xmin\").text)\n",
    "    ymin = int(obj.find(\"ymin\").text)\n",
    "    xmax = int(obj.find(\"xmax\").text)\n",
    "    ymax = int(obj.find(\"ymax\").text)\n",
    "\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "\n",
    "def generate_label(obj):\n",
    "    if obj.find(\"name\").text == \"with_mask\":\n",
    "        return 1\n",
    "    elif obj.find(\"name\").text == \"mask_weared_incorrect\":\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def generate_target(image_id, file):\n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, \"xml\")\n",
    "        objects = soup.find_all(\"object\")\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([image_id])\n",
    "        # Annotation is in dictionary format\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = img_id\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d41e31-17b6-4383-b7bb-8f3001b06ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:12:03.733334Z",
     "iopub.status.busy": "2022-02-07T14:12:03.732683Z",
     "iopub.status.idle": "2022-02-07T14:12:03.738575Z",
     "shell.execute_reply": "2022-02-07T14:12:03.737957Z",
     "shell.execute_reply.started": "2022-02-07T14:12:03.733289Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs = sorted(os.listdir(\"archive/images/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2dc9ffd-c8b8-4d41-b4f1-5b62c8f0e299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:12:43.099271Z",
     "iopub.status.busy": "2022-02-07T14:12:43.098966Z",
     "iopub.status.idle": "2022-02-07T14:12:43.123903Z",
     "shell.execute_reply": "2022-02-07T14:12:43.114898Z",
     "shell.execute_reply.started": "2022-02-07T14:12:43.099246Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = sorted(os.listdir(\"archive/annotations/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "328a4040-991a-4829-bc40-2e7ddec79e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:16:47.735711Z",
     "iopub.status.busy": "2022-02-07T14:16:47.735166Z",
     "iopub.status.idle": "2022-02-07T14:16:47.746866Z",
     "shell.execute_reply": "2022-02-07T14:16:47.746089Z",
     "shell.execute_reply.started": "2022-02-07T14:16:47.735658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = sorted(os.listdir(\"archive/images/\"))\n",
    "\n",
    "    #         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image = \"maksssksksss\" + str(idx) + \".png\"\n",
    "        file_label = \"maksssksksss\" + str(idx) + \".xml\"\n",
    "        img_path = os.path.join(\"archive/images/\", file_image)\n",
    "        label_path = os.path.join(\"archive/annotations/\", file_label)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # Generate Label\n",
    "        target = generate_target(idx, label_path)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e52196-05e2-464f-bbbb-e52a04bee8fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:17:33.877700Z",
     "iopub.status.busy": "2022-02-07T14:17:33.877012Z",
     "iopub.status.idle": "2022-02-07T14:17:33.884258Z",
     "shell.execute_reply": "2022-02-07T14:17:33.883296Z",
     "shell.execute_reply.started": "2022-02-07T14:17:33.877650Z"
    }
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c219bda6-afca-4606-a167-0ed723039a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:18:10.255208Z",
     "iopub.status.busy": "2022-02-07T14:18:10.254666Z",
     "iopub.status.idle": "2022-02-07T14:18:10.266806Z",
     "shell.execute_reply": "2022-02-07T14:18:10.265703Z",
     "shell.execute_reply.started": "2022-02-07T14:18:10.255179Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "dataset = MaskDataset(data_transform)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=4, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc80b04b-e6c0-412d-a7da-0ea96fe19eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:18:16.480532Z",
     "iopub.status.busy": "2022-02-07T14:18:16.480229Z",
     "iopub.status.idle": "2022-02-07T14:18:16.486550Z",
     "shell.execute_reply": "2022-02-07T14:18:16.485621Z",
     "shell.execute_reply.started": "2022-02-07T14:18:16.480507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e5a618-e961-4b61-bc3e-84c3874c9983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:18:48.089803Z",
     "iopub.status.busy": "2022-02-07T14:18:48.089485Z",
     "iopub.status.idle": "2022-02-07T14:18:48.095518Z",
     "shell.execute_reply": "2022-02-07T14:18:48.094368Z",
     "shell.execute_reply.started": "2022-02-07T14:18:48.089775Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        pretrained=True\n",
    "    )\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f919a53-9d9d-4ec8-8402-b8578c757e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:19:50.130722Z",
     "iopub.status.busy": "2022-02-07T14:19:50.129979Z",
     "iopub.status.idle": "2022-02-07T14:19:55.448385Z",
     "shell.execute_reply": "2022-02-07T14:19:55.448045Z",
     "shell.execute_reply.started": "2022-02-07T14:19:50.130681Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /Users/siowcm/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5202e642f04f718eef691790ea8601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "250fe8e0-10d3-4ae6-84df-21add8a45eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:20:09.859852Z",
     "iopub.status.busy": "2022-02-07T14:20:09.859324Z",
     "iopub.status.idle": "2022-02-07T14:20:09.951576Z",
     "shell.execute_reply": "2022-02-07T14:20:09.951319Z",
     "shell.execute_reply.started": "2022-02-07T14:20:09.859799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 79., 105., 109., 142.],\n",
      "        [185., 100., 226., 144.],\n",
      "        [325.,  90., 360., 141.]]), 'labels': tensor([0, 1, 0]), 'image_id': tensor([0])}, {'boxes': tensor([[321.,  34., 354.,  69.],\n",
      "        [224.,  38., 261.,  73.],\n",
      "        [299.,  58., 315.,  81.],\n",
      "        [143.,  74., 174., 115.],\n",
      "        [ 74.,  69.,  95.,  99.],\n",
      "        [191.,  67., 221.,  93.],\n",
      "        [ 21.,  73.,  44.,  93.],\n",
      "        [369.,  70., 398.,  99.],\n",
      "        [ 83.,  56., 111.,  89.]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0]), 'image_id': tensor([1])}, {'boxes': tensor([[ 68.,  42., 105.,  69.],\n",
      "        [154.,  47., 178.,  74.],\n",
      "        [238.,  34., 262.,  69.],\n",
      "        [333.,  31., 366.,  65.]]), 'labels': tensor([1, 1, 1, 2]), 'image_id': tensor([2])}, {'boxes': tensor([[ 52.,  53.,  73.,  76.],\n",
      "        [ 72.,  53.,  92.,  75.],\n",
      "        [112.,  51., 120.,  68.],\n",
      "        [155.,  60., 177.,  83.],\n",
      "        [189.,  59., 210.,  80.],\n",
      "        [235.,  57., 257.,  78.],\n",
      "        [289.,  60., 309.,  83.],\n",
      "        [313.,  68., 333.,  90.],\n",
      "        [351.,  35., 364.,  59.]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([3])}]\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "for imgs, annotations in data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [\n",
    "        {k: v.to(device) for k, v in t.items()} for t in annotations\n",
    "    ]\n",
    "    print(annotations)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec9e84-ba14-4a60-af47-414b4c2a2c74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:22:00.995837Z",
     "iopub.status.busy": "2022-02-07T14:22:00.994549Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siowcm/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "model.to(device)\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, lr=0.005, momentum=0.9, weight_decay=0.0005\n",
    ")\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [\n",
    "            {k: v.to(device) for k, v in t.items()} for t in annotations\n",
    "        ]\n",
    "        loss_dict = model([imgs[0]], [annotations[0]])\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        #         print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "        epoch_loss += losses\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac0ae1-7f1a-4215-a534-bf256284ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, annotations in data_loader:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041bd55-7735-4741-9ce4-b48f36b9b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = model(imgs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f74df-97d5-45cc-bca7-f9b92f5fc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img_tensor, annotation):\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    img = img_tensor.cpu().data\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    \n",
    "    for box in annotation[\"boxes\"]:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fcef6-350e-43c6-b584-2790b17f8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction\")\n",
    "plot_image(imgs[2], preds[2])\n",
    "print(\"Target\")\n",
    "plot_image(imgs[2], annotations[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e4bef9-d69b-4ae8-9e04-6a269c0953cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7c546-21b8-42a8-8b1a-96a2e4b74077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040daee-03f7-46d2-9e8f-a6620e4e68bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60534a9d-cd76-41bd-bd27-79256462532c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd88e1-d77d-4c2a-a751-c00a74af37bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
